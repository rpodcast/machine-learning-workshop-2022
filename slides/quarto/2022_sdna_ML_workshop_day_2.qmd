---
title: "GSS / SDnA ML"
subtitle: "Workshop Day 2: ML Lecture"
author: "Sydeaka Watson"
format: 
  revealjs:
    code-link: true
    code-line-numbers: true
    code-tools: true
    code-copy: true
    code-fold: true
    code-summary: "Show the code"
    smaller: false
    incremental: false
    scrollable: true
    theme: black
    footer: "2022 SDnA Machine Learning Workshop"
    slide-number: true
    chalkboard: true
    multiplex: true
    fig-height: 8
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Outline

-   Introduction

-   Algorithms

-   Tools

-   Principles and best practices

-   Resources

## This workshop is\...

-   A **gentle introduction** to machine learning concepts, algorithms, tools

-   An opportunity to use popular ML tools on real datasets for clinicalÂ and pre-clinical use-cases

## This workshop is NOT\...

-   A **comprehensive** overview of all machine learning concepts, algorithms, tools, use cases, etc.

-   For more in-depth study, I recommend that you review the supplementary materials linked at the end of the Day 2 and Day 3 slides.

-   If interested in learning about more advanced ML use cases, AADS invites you to attend the upcoming AADS AI / ML workshop later this fall (contact Haoda Fu for details)

## About Me

-   **Current role**: Senior Advisor - Innovative Sciences in SDnA/GSS

-   **Education**: Ph.D. Statistics, Baylor University, 2011

-   **Experience**:

    -   Biostatistician -- Los Alamos National Laboratory, University of Chicago Medicine

    -   Data Scentist -- AT&T, Elicit Insights (consulting firm), Eli Lilly (2020-present)

-   **Skills**: R / Python; Statistics; Machine Learning; Shiny app development

-   **Current residence**: Dallas, Texas

# Introduction

## Machine learning defined

There are many ways to define Machine Learning. Here's a good working definition: ![](images/ML_defined_by_MS.png)

::: footer
Source: <https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-machine-learning-platform/>
:::

::: notes
-   ML uses mathematical models to learn patterns without explicitly telling it how to learn.
-   It learns from experience
-   More data --\> more experience -- \> more likely that it will learn the patterns
:::

## ML vs Statistics

![](images/ML_vs_stats.png)

::: footer
Source: <https://towardsdatascience.com/how-is-machine-learning-different-from-statistics-and-why-it-matters-5a8ed539976>
:::

::: notes
-   Using numerical optimization instead of an assumed data generating process (Normal iid, etc)
-   More emphasis on predictive accuracy, less on inference
-   ML models don't lean on distributional assumptions. They require more data to learn the patterns
-   In statistical inference, we might care more about whether certain parameters are significant, and whether the our model assumptions are reasonable via Goodness of fit test. In ML, we care more about whether the model predictions are accurate.
-   ML models are less interpretable
-   R is the preferred tool for statisticians, whereas Python is the preferred tool for data scientists. However, the lines are blurring these days, so that is quite possible to do both stats and ML in both.
:::

## Healthcare applications

![](images/ML_healthcare.jpeg)

::: footer
Source: <https://www.botreetechnologies.com/blog/machine-learning-in-healthcare/>
:::

::: notes
-   Lots of uses for ML in healthcare
-   On Day 1 of the workshop, Haoda Fu in AADS mentioned the computer vision and
-   Yushi Liu in Discovery also talked about how he used ML for target identification
-   As you can see, there are lots of other applications that can help with operations, such as patient flow optimization and detecting process inefficiencies.
-   In the pre-clinical space, it could help with risk stratification, say for example if you wanted to identify high risk patients that could potentially get the most benefit from your molecule.
:::

## ML Benefits / Opportunities

-   All the cool kids are doing it ðŸ˜Ž

-   Can often yield more accurate predictions

-   Automatically learns interactive relationships between predictors and outcomes

    -   Less of a need to manually code interactions

-   Many approaches inherently include variable selection

-   Requires fewer assumptions than statistical models

    -   Independent variables

    -   Independent training examples

::: notes
-   AI/ML is the buzz word of the day, so everyone wants to be one of the folks who say they are doing AI/ML
-   The model predictions can be more accurate compared to some of the statistical modeling approaches
-   You wouldn't have to manually code interactions. It automatically learns how the predictors work together
-   The models usually help you understand which variables are most important in predicting the outcome
-   We don't lean on the types of assumptions that are used in traditional statistical models, for example, independent predictor variables or independent records (rows)
:::

## ML Challenges

-   Requires larger datasets

-   Garbage in \--\> Garbage out

    -   Can learn and re-enforce biases inherent in data

-   Must take care to avoid overfitting

-   Can take more time or computational resources to train the model

-   Steep learning curve for some

-   Must tune hyperparameters to get the best fit

    -   Automated ML can help with this (somewhat)

::: notes
-   As stated in previous slids, ML algorithms tend to require larger datasets
-   If the data contain biases, then it can pick up on those patterns and perpetuate those biases... so we have to be careful about this.
-   Can learn patterns in the training data that don't generalize well to other data (overfitting)
-   We are used to fitting a statistical model in a matter of seconds. However, some ML models can take several minutes to train (or hours, in extreme cases depending model complexity). May need to use more computational resources such as (1) multiple cores on a laptop (2) a high performance computing cluster or (3) a GPU.
-   May take a bit more time for some to learn the approach and feel comfortable applying these on real problems. However, hopefully after Days 2 and 3 of the workshop you will have a head start!
-   An algorithm's performance heavily depends on the values of the parameter values. We need to try lots of combinations of parameter values in order to find the optimal combination that will give the best fit. There are automated parameter search algorithms that can help with this.\
:::

## ML types

![](images/ML_types.png)

::: footer
Source: <https://litslink.com/blog/an-introduction-to-machine-learning-algorithms>
:::

::: notes
-   Three large classes of ML: supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning
-   Supervised means we have labeled data. We know "the answer" (the true value of the outcome), so we can assess how well the model correctly predicts the outcome.
    -   Regression - predict a continuous outcome such as housing prices
    -   Classification - decide which class an item or person belongs to. In medical imaging, we might want to decide whether the radiation image shows a tumor or no tumor.
-   Unsupervised means we don't have any labels, so there is no right or wrong answer that we want to predict. We want the algorithm to reveal patterns in the data, such as how units are grouped together, or how they are associated with one another.
-   Supervised and unsupervised learning are the most popular two types that you will encounter. The other two are beyond the scope of this lecture, but I can discuss these in a future workshop if there's interest.
:::

## Supervised Learning ML algorithms

-   Decision tree

-   Random forest

-   XGBoost

-   K Nearest Neighbors

-   Many more!

::: notes
-   I've listed several of the more popular types of ML algorithms for supervised learning ML
-   They are quite different in how they are implemented, have various strengths/weaknesses.
:::

## Unsupervised learning ML algorithms

-   K-means

-   K-protoypes

-   K Nearest Neighbors

-   Many more!

::: notes
-   These are a few popular unsupervised learning ML algorithms.
-   Probably not productive for us to go into the details of how each algorithm works in this session, so we can also host a future deep dive into the various algorithms in a future session if you all are interested.
:::

# Tools

## Integrated development environments (IDEs)

-   RStudio

    -   RStudio Server

    -   RStudio Desktop

    -   RStudio on Linux remote desktop

-   JupyterHub

::: notes
-   You might choose to use an IDE, or integrated development environment to do your ML modeling
-   These provide a framework for loading your R projects or python virtual environments, doing your exploratory data analysis, building your models, deploying your models, etc
-   RStudio works with both R and Python
-   JupyterHub works with both R and Python
-   Since the workshop is primarily aimed at statisticians in SDnA / GSS, and they mostly use R, I chose to demonstrate an R-based modeling demo for the workshop.
:::

## Data munging & EDA

These packages are used for data manipulation and exploratory data analysis

|      R      |         Python          |
|:-----------:|:-----------------------:|
| `tidyverse` |        `pandas`         |
|  `ggplot2`  | `matplotlib`, `seaborn` |

::: notes
-   R packages and python modules that are commonly used for data manipulation and exploratory data analysis
:::

## Model development

|      R       |  Python   |
|:------------:|:---------:|
| `tidymodels` | `sklearn` |
|    `h2o`     |   `h2o`   |

::: notes
-   R packages and python modules that are commonly used for ML model development
-   I reallly like the h2o modeling framework. Works with both R and python. Has a nice interface (command line + GUI). Lots of great utilities for model assessment and model deployment. It was my go-to ML framework prior to the tidymodels launch.
-   Tidymodels was developed more recently by the RStudio team. Max Kuhn is one of the authors. It is becoming more popular as they add new features and expand the user base. I find it to be well-developed. It is very intuitive and easy to learn. I highly recommend it.
-   We will use the tidymodels framework for the workshop.
:::

# ML best practices

## 10 ML best practices

::: columns
::: {.column width="50%"}
1.  Define modeling objectives

2.  Exploratory data analysis

3.  Feature engineering

4.  Feature selection

5.  Split data into training / testing sets
:::

::: {.column width="50%"}
6.  Train / validate the model

7.  Create visual summaries

8.  Explain the model predictions

9.  Deploy the model

10. Create a ML workflow
:::
:::

::: notes
-   As I reflected on the machine learning projects that I've worked on over the years, I came up with a list of 10 best practices for model development.
-   This isn't to suggest that these are to be performed in this particular order.
-   You may end up doing and re-doing some or all of these steps multiple times as the project progresses.
-   We will spend the rest of the workshop diving into each of these principles
:::

# 1. Define modeling objectives

## 1. Define modeling objectives

-   What do we want the model to do?

    -   Classification? Regression? Clustering?

-   What is the outcome / target variable (if any)?

-   Identify the relevant data sources

-   Select appropriate ML algorithm(s) that align with the modeling objective

::: notes
-   As in a statistical analysis, we should start with a plan.
-   What do we want the model to do... classification? Regression?
-   If it's a supervised learning project, what is the outcome that we want to predict?
-   Which data sources are we going to leverage?
-   Which ML algorithms do we want to try?
:::

## 1. Example: Clinical Trial ACTG 175

-   ACTG 175 was a randomized, double-blind, placebo-controlled trial that included HIV-1--infected subjects with CD4 cell counts 200-500 per cubic mm

-   Treatments

    -   Monotherapy: (1) zidovudine alone or (2) didanosine alone
    -   Combination therapy: (3) zidovudine + didanosine or (4) zidovudine + zalcitabine

-   Primary study end point: (1) \>50% decline in CD4 cell count, (2) progression to AIDS, or (3) death.

## 1. Example: Modeling Plan

-   Objective: Given baseline characteristics, predict whether patient will experience the primary study endpoint within the observation period

-   Outcome: Binary indicator of whether patient had \>50% CD4+ increase, progressed to AIDS, or death

-   Data source: Clinical data -- baseline predictors, treatment assignment, study outcomes

-   Candidate algorithms

    -   Supervised learning / binary classification: Random forest, XGBoost, & Logistic regression

::: notes
-   Try two ML approaches: random forest and xgboost
-   Compare to a simple logistic regression as a benchmark.
-   Do this problem really need a ML solution? Do we see any benefit beyond what we would get from a simpler model?
:::

# 2. Exploratory data analysis

## 2. Exploratory data analysis

-   Data quality

-   Missing data summary / investigation

-   Determine if we need to exclude any data points or variables

-   Determine if we need to transform any variables

-   Examine relationships between candidate predictors and outcomes

## 2. Example: Environment setup

::: panel-tabset
### Load Packages

```{r load_pkg}
#| echo: fenced
#| code-fold: false

# Load the packages
suppressPackageStartupMessages(library(speff2trial))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(testthat))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(yardstick))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(themis))
suppressPackageStartupMessages(library(DALEXtra))
```

### Load Helper Fns

```{r load_helpers}
#| echo: fenced
#| code-fold: false

# Helper function: Pretty data table
# Table fmt ref: https://stackoverflow.com/questions/44368572/dt-showing-more-rows-in-dt
pretty_dt <- function(tbl, num_digits = 3, num_rows=20) {
  dt <- tbl %>% 
    head(num_rows) %>%
    datatable(
      fillContainer = FALSE, 
      options = list(pageLength = 5, 
                     autoWidth = TRUE,
                     dom = 'tip', # table only
                     scrollX = TRUE,
                     scrollY = TRUE,
                     initComplete = JS(
                            "function(settings, json) {",
                            "$(this.api().table().header()).css({'color': 'white'});",
                            "}")
        )
      )
  
  # Round float columns (numeric but not integer)
  cols_numeric <- colnames(tbl)[sapply(tbl, is.numeric)]
  cols_integer <- colnames(tbl)[sapply(tbl, is.integer)]
  cols_float <- cols_numeric[!(cols_numeric %in% cols_integer)]
  if (length(cols_float) > 0) {
    dt <- dt %>% formatRound(cols_float, num_digits)
  }
  
  return(dt)
}
```

### Load Data

```{r load_data}
#| echo: fenced

# Load the data
data(ACTG175)
dat_raw <- ACTG175
dat_raw %>% summary
```
:::

::: notes
-   To start, we set up the environment. Load packages and a helper function that makes it easier to display tables in the slides.
-   Clinical data for the ACTG 175 trial comes from the `speff2trial` R package
-   The help page for this dataset includes a data dictionary that shows how the data are coded. It would be helpful to transform the data using these suggested codings.
:::

## 2. Example: Data transformations

```{r data_transformations}
dat_clean <- dat_raw %>%
  mutate(treatment_arm = case_when(
    arms == 0 ~ 'zidovudine',
    arms == 1 ~ 'zidovudine and didanosine',
    arms == 2 ~ 'zidovudine and zalcitabine',
    arms == 3 ~ 'didanosine',
    TRUE ~ NA_character_
  )) %>%
  mutate(
    hemo = case_when(
      hemo == 0 ~ 'no',
      hemo == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    lgbtq = case_when(
      homo == 0 ~ 'no',
      homo == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    hist_intra_drug_use = case_when(
      drugs == 0 ~ 'no',
      drugs == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_nz_art = case_when(
      oprior == 0 ~ 'no',
      oprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_z_30days = case_when(
      z30 == 0 ~ 'no',
      z30 == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_z = case_when(
      zprior == 0 ~ 'no',
      zprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    days_prior_art = preanti,
    race = case_when(
      race == 0 ~ 'white',
      race == 1 ~ 'non-white',
      TRUE ~ NA_character_
    ),
    gender = case_when(
      gender == 0 ~ 'female',
      gender == 1 ~ 'male',
      TRUE ~ NA_character_
    ),
    prior_art = case_when(
      str2 == 0 ~ 'naive',
      str2 == 1 ~ 'experienced',
      TRUE ~ NA_character_
    ),
    strat_hist_art = case_when(
      strat == 1 ~ 'antiretroviral naive',
      strat == 2 ~ '> 1 but <= 52 weeks of prior antiretroviral therapy',
      strat == 3 ~ '> 52 weeks',
      TRUE ~ NA_character_
    ),
    symptom = case_when(
      symptom == 0 ~ 'asymptomatic',
      symptom == 1 ~ 'symptomatic',
      TRUE ~ NA_character_
    ),
    prior_z = case_when(
      zprior == 0 ~ 'no',
      zprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_z = case_when(
      zprior == 0 ~ 'no',
      zprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    zidovudine_indicator = treat,
    surv_days = days,
    surv_event = cens,
    event = ifelse(surv_event == 0, 'no', 'yes') %>% factor,
    baseline_cd4 = cd40,
    baseline_cd8 = cd80
  ) %>%
  select(
    # identifier
    pidnum, 
    # treatment assignment 
   zidovudine_indicator, treatment_arm, offtrt, 
    # baseline predictors
    age, wtkg, hemo, lgbtq, hist_intra_drug_use, karnof, prior_nz_art, prior_z_30days, prior_z, days_prior_art, race, gender, prior_art, strat_hist_art, symptom, baseline_cd4, baseline_cd8, 
   # survival outcome
   event, surv_event, surv_days,
   # other outcomes
   cd420, cd496, r, cd820
    ) %>%
  mutate_if(is.character, factor) 

dat_clean %>% summary
```

::: notes
-   Used the data dictionary to recode the categorical variables into meaningful labels
-   Changed some variable names
-   Selected only the relevant columns that I might want to use as predictors or outcomes
-   Represented character fields as factor
-   The data summary is easier to read after applying transformations
:::

## 2. Example: Data preview

```{r data_preview}
dat_clean %>% pretty_dt
```

::: notes
-   Showing the first few rows of the transformed data table
:::

## 2. Example: Data visualizations

::: panel-tabset
### Plots 1 - 2

```{r data-data_viz_p12, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Event distribution is skewed"
#|   - "Baseline CD4 levels extend beyond the 200-500 range"

# Specify variable types
var_outcomes <- c('event', 'surv_event', 'surv_days', 'cd420', 'cd496', 'r', 'cd820')
excluded_cols <- c('pidnum', 'zidovudine_indicator', 'offtrt')
classes <- sapply(dat_clean, class)
vars_factor <- classes[classes == 'factor'] %>% 
  names %>% 
  .[!(. %in% c('progression', 'event'))]
vars_cont <- classes[(classes  %in% c('integer', 'numeric'))] %>% 
  names %>% 
  .[!(. %in% c(var_outcomes, excluded_cols))]


# Bar plot of event counts
dat_clean %>%
  count(event) %>%
  ggplot(aes(x = event, y = n)) + 
  geom_bar(stat = 'identity') + 
  ylab('') + ggtitle('Patient counts by event status')


# Boxplot of baseline CD4 levels across treatment arms
dat_clean %>%
  ggplot(aes(x = treatment_arm, y = baseline_cd4)) + 
  geom_boxplot() + 
  coord_flip() +
  xlab('') + ylab('') + ggtitle('Baseline CD4')
```

### Plots 3 - 4

```{r data_viz_p34, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "All patients have had prior zidovudine therapy "
#|   - "Few patients have had prior non-zidovudine therapy "

# Bar plot of prior_z counts
dat_clean %>%
  count(prior_z) %>%
  ggplot(aes(x = prior_z, y = n)) + 
  geom_bar(stat = 'identity') +
  xlab('') + ylab('') + ggtitle('Patient counts by prior zidovudine history')


# Bar plot of prior_nz_art counts
dat_clean %>%
  count(prior_nz_art) %>%
  ggplot(aes(x = prior_nz_art, y = n)) + 
  geom_bar(stat = 'identity') +
  xlab('') + ylab('') + ggtitle('Patient counts by prior non-zidovudine history')
```
:::

::: notes
-   As in any other data analysis, it is helpful to prepare summary plots that show distributions and trends in the data.
-   The event distribution is skewed. The event only occurs in about 25% of the patients. We will come back to this point.
-   Inclusion criteria in the paper says the study included patients with CD4 cell count values ranging from 200-500 per cubic mm. However, the baseline CD4 ranges extend beyond that range in all treatment arms. There is even a patient who appears to have a baseline CD4 of zero. This could either mean that we have data quality issues, and/or that some patients may have had significant shifts in their CD4 values since they were initially assessed.
-   All of the patients have the same value in the prior zidovudine variable. This suggests that all patients had zidovudine treatment in the past.
-   We also see that only a few patients had no prior non-zidovudine treatment in the past. There is very little variation in these variables, so they probably won't be useful as predictor variables in a model.
:::

# 3. Feature engineering

## 3. Feature engineering

-   Create new features that extract additional insight from existing features

-   For example, we could split timestamp "2022-09-23 11:37:48 CDT" into:

    ::: columns
    ::: {.column width="50%"}
    -   year = 2022

    -   quarter = 3

    -   month = 9

    -   day_of_month = 23

    -   day_of_week = 'Friday'
    :::

    ::: {.column width="50%"}
    -   hour_24 = 11

    -   minutes = 37

    -   seconds = 48

    -   time_zone = 'CDT'
    :::
    :::

::: notes
-   In ML, predictor variables are called "features"
-   Feature engineering is where we take existing variables and create new ones that extract additional information
-   This allows us to "help" the model understand what we want it to know about a specific var
-   As an example, this timestamp wouldn't be helpful as model input.
-   We can split this field into multiple columns that include date components that the model might want to know about.
:::

## 3. Example: Individual drugs

```{r feat_eng}
dat_clean <- dat_clean %>%
  mutate(
    ind_didanosine = ifelse(grepl(pattern='didanosine', x=treatment_arm), 1L, 0L),
    ind_zidovudine = ifelse(grepl(pattern='zidovudine', x=treatment_arm), 1L, 0L),
    ind_zalcitabine = ifelse(grepl(pattern='zalcitabine', x=treatment_arm), 1L, 0L)
  )

dat_clean %>%
  distinct(treatment_arm, ind_didanosine, ind_zidovudine, ind_zalcitabine) %>%
  pretty_dt
```

::: notes
-   As an example, notice that three treatments contain zidovudine and two contain didanosine.
-   If we use treatment arm as a predictor in its current form, the model wouldn't understand that some of the treatments contain the same drug
-   We might help the model by creating indicator variables for the individual drugs
-   The table on this slide shows how the treatment arms are recoded
-   This creates some redundancy, though. The treatment arm is completely specified by the indicators. If you know the values of all the individual drug indicators, you know which treatment arm. This could cause some issues in statistical methods. We will see if we run into any issues in the ML model.
:::

# 4. Feature selection

## 4. Feature selection

-   Which variables do we want to include in the analysis?

-   Which should be excluded?

    -   Identifier fields, redundant variables, zero-variance variables, etc.

    -   Keep all variables and let the algorithm figure it out? Or feed it specific variables?

-   Variable reduction - principal components

-   Could also choose n top variables from preliminary model fit

::: notes
-   The next step is to decide which variables we want to include or exclude from the model
-   This includes the original variables and the new ones we introduced in the feature engineering step
-   We want to keep the features that think are relevant to the modeling objective
-   Exclude features that are redundant (that provide information that other variables already capture)
-   Exclude features that have zero variance (the same value for all subjects)
-   After that first cut, we could still be left with hundreds of candidate predictors. Do we keep them all, or try to further reduce the number of variables
-   Might use a dimension reduction approach such as principal components.
-   Could also use ML to help with this task. Fit a preliminary ML model, select the top n important features identified in that model.
:::

## 4. Example: Keeping these variables

-   Treatment arm assignments (factor + binary indicators)

-   Baseline characteristics

    -   Age, weight, race, gender, sexual orientation, etc.

    -   Treatment history (including stratification variable)

    -   CD4 and CD8 counts

-   Symptomatic vs asymptomatic?

-   Karnofsky score

::: notes
-   Here's a summary of the variables that I thought might be relevant in the prediction of the primary endpoint
:::

## 4. Example: Excluding these variables

-   [**pidnum**]{.underline}: patient identifier shouldn't be used as a predictor

-   [**offtrt**]{.underline}: we wouldn't know this at baseline (whether the patient went off treatment)

-   [**prior_z**]{.underline}: all patients have prior zidovudine treatment (i.e., all patients have the same value for this variable), so there's no value add

-   [**prior_nz_art**]{.underline}: Very little variability in this variable (2092 no vs 47 yes)

-   [**CD4 and CD8 measurements at later time points**]{.underline}: we wouldn't know these at baseline

::: notes
-   Here's a list of variables I thought we should exclude.
    -   Patient identifier numbers
    -   Things we wouldn't know at baseline, such as whether the patient went off treatment, and future CD4 and CD8 measurements
    -   The two treatment history variables that had little or no variability
:::

## 4. Example: Analysis subset

```{r var_types}
var_predictors <- c(
    # treatment assignment 
    'treatment_arm', 
    'ind_didanosine', 'ind_zidovudine', 'ind_zalcitabine',
    # baseline predictors
    'age', 'wtkg', 'hemo', 'lgbtq', 'hist_intra_drug_use', 'karnof', 'prior_z_30days', 
    'days_prior_art', 'race', 'gender', 'prior_art', 'strat_hist_art', 'symptom', 'baseline_cd4', 'baseline_cd8')

var_outcomes <- c('event', 'surv_event', 'surv_days', 'cd420', 'cd496', 'r', 'cd820')
vars_all <- colnames(dat_clean)
vars_to_keep <- c(var_predictors, var_outcomes)
vars_excluded <- vars_all[!(vars_all %in% vars_to_keep)]

dat_analysis <- dat_clean %>% select_at(vars_to_keep)

dat_analysis %>% summary
```

::: notes
-   I applied these variable inclusions and exclusions so that we only keep the relevant columns in the dataset
-   I kept a few additional outcome fields that we may need in Day 3 of the workshop. These include survival outcomes and future CD4 and CD8 measurements.
-   Here's a summary of the dataset that we will use for the analysis.
:::

# 5. Split data into training / testing

## 5. Split data into training / testing

-   Fit the model on the training set(s)

-   Get intermediate assessment of model performance on validation set(s)

-   Use k-fold cross-validation to train/validate on multiple data splits

-   Get final assessment of model performance on testing set

::: notes
-   In statistical inference, we usually use the entire dataset to fit the model
-   However, in prediction, we need to set some data aside to use as a sanity check. We need to see how well the model works on data that it has never seen before.
-   We can have data split into 3 parts... training, validation, and testing
    -   Training: fit the model
    -   Validation: intermediate assessment of model performance. Used to compare model fits across algorithms or parameter settings. Technically still used in training since we are using it to select the model, so it isn't a completely separate dataset.
    -   Testing: Use this to get a final assessment of model performance after all modeling is complete
    -   Problem with this approach is that we can get an unlucky split. May get different results depending on which cases ended up in training vs validation set
-   Another approach: k-fold cross-validation
    -   Split the data into training and testing sets
    -   Further split the training data into k folds
    -   Fit the model k times, using (k-1) folds train the model and the held out kth fold for assessment
    -   Average across folds
    -   Gives better assessment of how the model will perform on external data (generalizable) \_ I recommend the latter approach
:::

## 5. Example: Train /validation/test split

### Initial data split

85 % training / validation + 15% test

```{r dat_split}
# Set the seed so the result is reproducible
set.seed(123)
# 85% training / 15% testing
dat_split <- initial_split(dat_analysis, prop = 0.85, strata = strat_hist_art)
dat_split
```

```{r dat_split_dims}
dat_train_and_val <- training(dat_split)
dat_test <- testing(dat_split)

df_dimensions <- list(
  full_dataset = dat_analysis %>% dim,
  train_val_dataset = dat_train_and_val %>% dim,
  test_dataset = dat_test %>% dim
)

df_dim_message <- sapply(names(df_dimensions), function(df_name) {
  glue::glue("`{df_name}` contains {df_dimensions[[df_name]][1]} rows and {df_dimensions[[df_name]][2]} columns.")
}) %>%
  paste(., collapse='\n')

cat(df_dim_message)
```

::: notes
-   Here's our initial data split.
    -   85% of the data will be used for training and validation across k folds.
    -   The remining 15% will be used for final testing
-   Summary at the bottom shows how the two splits add up to the size of the entire dataset
-   If we are using the k-fold cross validation approach, this is the data split that we would use.
:::

------------------------------------------------------------------------

### Correct class imbalance issue

-   There are 3x as many 'no' events as there are 'yes' events.

-   This can negatively impact model performance.

```{r}
# Show imbalanced distribution of event outcome
dat_train_and_val %>% count(event)
```

::: notes
-   I mentioned earlier in the data visualization section that there was an issue with the event column.
-   There are 3x as many 'no' events as there are 'yes' events.
-   This is called a class imbalance.
-   Remember ML models learn from experiences from reviewing example data.
-   The algorithm will have more examples of 'no' events than it does with 'yes' events, so it will do a better job predicting the 'no' cases.
:::

------------------------------------------------------------------------

### Correct class imbalance issue

-   We can use the `step_upsample` function to over-sample the minority class (i.e., the 'yes' class).

-   This creates replicate rows of the 'yes' records that can artificially make the proportions more balanced

-   We would only do this for the training data

```{r}
# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Recipe for upsampling the minority class
upsample_rec <- 
  recipe(model_formula, data = dat_train_and_val) %>%
  step_upsample(event, over_ratio = 0.7) %>%
  prep()

# Apply the upsampling
dat_train_and_val <- upsample_rec %>%
  bake(new_data = NULL) 

# Show new distrubution of event outcome
dat_train_and_val %>%  count(event)
```

::: notes
-   There is a function in the recipes framework called step_upsample that handles oversampling
-   After oversampling, now the event distribution is less imbalanced
-   This is only something that we would need to do for data that will be involved in training the model. No need to do this for the testing set.
:::

------------------------------------------------------------------------

### Validation split

80% training + 20% validation

```{r dat_val_split}
set.seed(321)
dat_val_split <- validation_split(dat_train_and_val, prop = 0.80, strata = strat_hist_art)

dat_train <- analysis(dat_val_split$splits[[1]])
dat_valid <- assessment(dat_val_split$splits[[1]])

df_dimensions <- list(
  dat_train_and_val_dataset = dat_train_and_val %>% dim,
  train_dataset = dat_train %>% dim,
  valid_dataset = dat_valid %>% dim
)


df_dim_message <- sapply(names(df_dimensions), function(df_name) {
  glue::glue("`{df_name}` contains {df_dimensions[[df_name]][1]} rows and {df_dimensions[[df_name]][2]} columns.")
}) %>%
  paste(., collapse='\n')

cat(df_dim_message)
```

::: notes
-   This step creates a separate validation set.
-   This is used if you didn't use k-fold cross validation, so that you have a pre-defined training and validation split.
-   We will see how both versions are used in the next section.
:::

# 6. Train the model(s)

## 6. Train the model(s)

-   Model = modeling algorithm + model formula + algorithm hyperparameter values

    -   Try different modeling algorithms

    -   Try different model formulas

    -   Try different hyperparameter values

    -   May need to use parallel processing over multiple cores for faster training

-   Model validation - evaluate each model on holdout dataset(s)

    -   Good: Train on training set + validate on validation set

    -   Best: Cross-validation on training/validation set

::: notes
-   Next step is to train the models
    -   Working definition: A model is a specific choice of modeling algorithm, the model formula, and the parameter values to be used in the algorithm
    -   Think of this as an experiment. Try different modeling algorithms, different model formulations, and different hyperparameter values.
    -   This can take a lot of time and computational resources to run, so we may need to use parallel processing over multiple cores if we want to speed up the training
-   We also need to validate the model to see how well it works on data it hasn't seen before.
-   To summarize my advice on previous slide
    -   At minimum, set aside some data in a validation set that you will use to evaluate your model
    -   However I highly recommend that instead of using a single validation split, that you run k-fold cross validation to get a better idea of how generalizable your model is
:::

## 6a. Example: Benchmark model

We will compare ML model performance to this basic logistic regression fit.

```{r}
# Model specification
model_spec_logreg <- logistic_reg(mode = "classification", engine = "glm")

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflow
logreg_model_wflow <- workflow(model_formula, model_spec_logreg)

# Train the model
model_fit_logreg <- fit(logreg_model_wflow, dat_train)

# Append the predictions to the testing dataframe
dat_valid_w_preds_logreg <- augment(model_fit_logreg, new_data = dat_valid) %>%
  mutate(.pred_yes_int = ifelse(.pred_yes > .5, 1, 0))

# Show the augmented validation dataframe (original data + predictions)
dat_valid_w_preds_logreg %>% pretty_dt
```

::: notes
-   Before fitting any ML models, I like to fit what is called a "benchmark" model
-   It is a simple model that probably isn't very good. However, it's used as a baseline that we will compare the ML model results to
-   In this case, I'm choosing to fit a very basic logistic regression model, using the same model formula and training data that I will use to fit the ML models
-   After fitting the model `augment` function appends the predicted probabilities of each category in the outcome class to the validation set. It also includes the predicted class, basically whichever class had the higher predicted probability.
:::

## 6a. Example: Benchmark model

Logistic regression model accuracy

```{r}
# Get model performance: Accuracy
dat_valid_w_preds_logreg %>%
  yardstick::accuracy(truth=event, estimate=.pred_class) %>%
  pretty_dt
```

::: notes
-   Accuracy = proportion of cases that were correctly classified.
-   Interpretation: higher accuracy --\> better performance
-   Model accuracy isn't very high in the logistic regression model
:::

## 6a. Example: Benchmark model

Area under ROC Curve for logistic regression model fit

```{r}
# Get model performance: Area under the ROC curve
dat_valid_w_preds_logreg %>%
  yardstick::roc_auc(truth=event, estimate=.pred_yes, event_level='second') %>%
  pretty_dt
```

::: notes
-   AUC = Area under the Receiver Operator Characteristic curve, or ROC curve
-   Interpretation: higher AUC --\> better performance
-   AUC isn't very high in the logistic regression model
:::

## 6a. Example: Benchmark model

Confusion matrix: Logistic regression model struggles to correctly predict events.

```{r}
cm_logreg <- dat_valid_w_preds_logreg %>%
  conf_mat(truth=event, estimate=.pred_class) %>%
  autoplot(type='heatmap')

cm_logreg
```

::: notes
-   Confusion matrix summaries how the predicted values compare to the actual values
-   Logistic regression model gives lots of false positives and false negatives
:::

## 6b. Example: Set up parallel backend

-   Machine learning algorithms can be computationally expensive

-   Parallelizing these computations over multiple cores can significantly reduce training time

-   Code below shows an example parallel backend setup

```{r parallel_setup}
#| echo: fenced
#| code-fold: false

cores <- parallelly::availableCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)
```

::: notes
-   Many ways to do this, but this code shows one example setup
:::

## 6c. Example: RF k-fold CV

Train a Random Forest model using k-fold cross-validation

-   Same model formula used in the logistic regression (benchmark model)

-   Using the Random Forest implementation provided in the `ranger` package

-   Setting specific values for selected hyperparameters (`trees`, `mtry`, and `min_n`)

------------------------------------------------------------------------

### Example: RF k-fold CV

-   In each step, fit models on (k-1) folds combined and evaluate performance on remaining fold

-   Fold splits are stratified by patient's history of anti-retroviral therapy

-   Saving predictions on holdout folds for later use

-   Default performance metrics for binary classification: accuracy + AUC

------------------------------------------------------------------------

### Code

```{r rf_train_w_cv}
#| echo: fenced
#| code-fold: false

# Generate k folds for CV
num_folds <- 5
dat_folds <- vfold_cv(dat_train_and_val, v = num_folds, strata = strat_hist_art)

# Save the assessment set results
ctrl <- control_resamples(save_pred = TRUE)

# Model specification
model_spec_rf <- rand_forest(mode = 'classification', engine = 'ranger', 
                             trees = 300, mtry = 10, min_n = 100)

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflow
rf_model_wflow <- workflow(model_formula, model_spec_rf)

# k-fold cross validation of the random forest model
fit_cv_res <- fit_resamples(rf_model_wflow, dat_folds, control = ctrl)
```

## 6c. RF k-fold CV

-   Evaluate model performance

    -   Aggregate performance metrics across folds

    -   Shows mean, standard error, and n = \# folds

```{r}
fit_cv_res %>% 
  collect_metrics() %>%
  pretty_dt()
```

## 6c. RF k-fold CV

-   Get predictions on the held-out folds

-   Predicted probability of yes or no

-   Predicted class = whichever has highest predicted probability

-   Also includes the true classification column `event` for comparison purposes

------------------------------------------------------------------------

```{r}
fit_cv_res %>% 
  collect_predictions() %>%
  pretty_dt()
```

## 6d. Example: Multiple models

-   Fit Random forest, XGBoost, and Logistic regression models in a single workflow

-   Can test various model formula and algorithm combinations

```{r multi_model_train_w_cv}
#| echo: fenced
#| code-fold: false

# Generate k folds for CV
num_folds <- 3
dat_folds <- vfold_cv(dat_train_and_val, v = num_folds, strata = strat_hist_art)

# Save the assessment set results
ctrl <- control_resamples(save_pred = TRUE)

# Model specifications
model_spec_rf <- rand_forest(mode = 'classification', engine = 'ranger', trees = 300, 
                             mtry = 10, min_n = 25)
#model_spec_dt <- decision_tree(mode = 'classification', engine = 'rpart', tree_depth = 5, 
#                               min_n = 20)
model_spec_xgb <- boost_tree(mode = 'classification', engine = 'xgboost', trees = 250,
                             tree_depth = 5, min_n = 30, learn_rate = 0.5)
model_spec_logreg <- logistic_reg(mode = "classification", engine = "glm")

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflows
rf_model_wflow <- workflow(model_formula, model_spec_rf)
#dt_model_wflow <- workflow(model_formula, model_spec_dt)
xgb_model_wflow <- workflow(model_formula, model_spec_xgb)
logreg_model_wflow <- workflow(model_formula, model_spec_logreg)

wf_set <- workflow_set(list(model_formula), 
                       list(model_spec_logreg, model_spec_rf, model_spec_xgb))

wf_set_fit <- wf_set %>%
  workflow_map("fit_resamples", resamples = dat_folds)
```

::: notes
-   Tidymodels provides a convenient framework for fitting multiple models
-   Here, we specify RF, XGB, and Logistic regression model using specific values of their hyperparameters
-   Put them all together into a single workflow set using the `workflow_set` function
    -   Could have used multiple model formulations, for example if we wante to explore different variable subsets
-   Run all combinations of model formulas and model specifications using the `workflow_map` function
:::

## 6d. Example: Multiple models

Leaderboard shows model performance rankings across modeling scenarios

```{r}
# Rank the sets of models by their aggregate metric performance
leaderboard <- wf_set_fit %>% rank_results()
leaderboard %>% pretty_dt()
```

::: notes
-   Leaderboard ranks model performance across the different modeling scenarios
-   Random forest is in first place, followed by XGBoost.
-   Both beat our benchmark logistic regression model
:::

## 6e. Example --- Param tuning w/ CV

Tune hyperparameters

-   Fit the same algorithm multiple times using different parameter values

-   Choose the parameter combination that optimizes model performance

-   Can choose grid search approach - Latin Hypercube, Max entropy, Regular, etc

-   Can choose parameter value ranges or have them automatically selected

::: notes
-   What I hear: "I tried random forest, but it didn't give great results."
-   What that usually means: "I tried one specific set of parameter values for random forest, and it didn't give great results."
-   Parameter value settings can have a significant impact on the model performance. We need to find the combination of parameters that work well or the specific modeling exercise.
-   Grid search = try a bunch of different parameter combinations; choose the set of values that optimizes the performance metric
    -   Several grid search approaches in tidymodels framework: latin hypercube, max entropy, regular, etc
    -   Can have the algorithm choose value ranges for you, or you could set them yourself
:::

------------------------------------------------------------------------

### Code

```{r xgb_param_tune_w_cv}
#| echo: fenced
#| code-fold: false

set.seed(9265)

# Generate k folds for CV
num_folds <- 5
dat_folds <- vfold_cv(dat_train_and_val, v = num_folds, strata = strat_hist_art)

# Model specifications
# Some parameters are specified, while others are marked for tuning 
model_spec_xgb_tune <- boost_tree(mode = 'classification', engine = 'xgboost', trees = tune(),
                             tree_depth = tune(), min_n = 54, learn_rate = tune(), 
                             mtry = tune())

# Save the assessment set results
ctrl <- control_grid(save_pred = TRUE, allow_par=TRUE, parallel_over = "everything", 
                     verbose=TRUE, event_level='second')

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflow
xgb_model_wflow_tune <- workflow(model_formula, model_spec_xgb_tune)

xgb_param <- 
  xgb_model_wflow_tune %>% 
  extract_parameter_set_dials() %>% 
  update(
    trees = trees(c(100, 1500)),
    learn_rate = learn_rate(c(.00005, .1), trans= NULL),
    tree_depth = tree_depth(c(4, 20)),
    mtry = mtry(c(3, 20))
  )

# Tune the hyperparameters using k-fold cross validation
set.seed(9)
tm <- system.time({
  xgb_res <-
    xgb_model_wflow_tune %>%
    tune_grid(resamples = dat_folds, 
              grid = xgb_param %>% grid_max_entropy(size = 100),
              control = ctrl)
})

# How long did it take to run?
print(tm)
```

::: notes
-   Example: tune parameters in the XGBoost algorithm
-   Using 5-fold cross validation to assess the performance of each parameter combination across folds
-   Use the tune() function to indicate that we want to train 4 important parameters in the XGBoost model
    -   number of trees, learning rate, tree depth, and mtry
-   In this example, I specifically set the parameter ranges that I want the grid search to focus on
-   And then I use the max entropy method to select 100 different parameter combinations
-   As you can see, it took some time to run
:::

## 6e. Example --- Param tuning w/ CV

Summarize model performance metrics across folds

```{r xgb_metrics}
xgb_res %>% 
  collect_metrics() %>%
  filter(.metric == 'roc_auc') %>%
  pretty_dt
```

::: notes
-   AUC metrics averaged across folds for each parameter combination in the grid search
:::

## 6e. Example --- Param tuning w/ CV

Show the top XGBoost model

```{r xgb_top_model}
# top model
xgb_res %>% 
  show_best(metric = 'roc_auc', n=1) %>% 
  pretty_dt
```

::: notes
-   The `show_best` function allows us to select the optimal parameter combination, i.e., the one that gives the highest AUC
:::

## 6e. Example --- Stop the cluster

Shut down the parallel backend

```{r parallel_close}
#| echo: fenced
#| code-fold: false

foreach::registerDoSEQ()
parallel::stopCluster(cl)
```

::: notes
Don't forget to shut down the cluster after you are done training your models!
:::

# 7. Create visual summaries

## 7. Create visual summaries

Visual model summaries can often provide additional insight

-   Compare model performance across modeling scenarios

-   Compare predicted values to observed

    -   Regression: scatterplot

    -   Classification: confusion matrix

## 7. Example: Model performance across tuning scenarios

```{r xgb_tune_metrics, fig.height=6}
xgb_tune_metrics <- xgb_res %>% 
  collect_metrics() %>%
  mutate(scenario = glue::glue("learn_rate = {signif(learn_rate, 3)}; trees = {trees}; tree_depth = {tree_depth}"))

plot_tune_metrics_xgb <- xgb_tune_metrics %>%
  #filter(.metric == 'roc_auc') %>% 
  ggplot(aes(x=scenario)) + 
  geom_point(aes(x=scenario, y=mean)) +
  geom_errorbar(aes(ymin=mean-std_err, ymax=mean+std_err), width=.2,
                 position=position_dodge(0.05)) +
  facet_wrap(~.metric) +
  coord_flip()

plot_tune_metrics_xgb
```

::: notes
-   Sorry for the small print!
-   Each row shows the mean +/- SE for the accuracy and AUC measurements for a specific parameter combination in the XGBoost grid search
-   Lots of variability in performance
:::

## 7. Example: ROC vs param value

-   Plotting the model performance against the parameter values in the grid can often reveal trends that can help refine the grid for better performance.

-   For example... Model performance tends to increase with learning rate

------------------------------------------------------------------------

### Plots: Learning rate and num. trees

```{r, fig.width=10, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Learning rate"
#|   - "Number of trees"

plot_param <- function(param) {
  xgb_tune_metrics %>%
    filter(.metric == 'roc_auc') %>%
    arrange_at('mean') %>%
    ggplot(aes_string(x=param, y='mean')) +
    geom_point() + 
    xlab(param) + 
    ylab('') + 
    ggtitle(glue::glue("ROC vs {param}"))
}

plot_param('learn_rate')
plot_param('trees')
```

::: notes
-   Learning rate = Curved relationship; variance increases with learning rate; best performance seems to occur with higher learning rate
-   Similar for number of trees
:::

------------------------------------------------------------------------

### Plots: Tree depth and mtry

```{r, fig.width=10, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Tree depth"
#|   - "Mtry"

plot_param('tree_depth')
plot_param('mtry')
```

::: notes
-   Tree depth and mtry don't seem to affect model performance as much here
:::

## 7. Example: Select the best model

-   Best model is the one that optimizes the performance metric

-   For example, we could choose the model with highest AUC

------------------------------------------------------------------------

### Code

```{r}
## XGBoost model that has the best AUC
best_auc <- select_best(xgb_res, metric = "roc_auc")

## Workflow associated with the best AUC
best_xgb_wflow <- xgb_model_wflow_tune %>% 
  finalize_workflow(best_auc)

## The final model fit
final_model_fit <- 
  best_xgb_wflow %>% 
  fit(data = dat_train)

print(final_model_fit)
```

::: notes
-   Scroll down to see the rest of the output
-   This view shows the details of the final XGBoost model fit using the optimal combination of parameters
:::

# 8. Explain model predictions

## 8. Explain model predictions

-   Understand the patterns that the model has learned
-   What does it understand about the relationship between the outcome and predictors?
    -   Any new insights?
    -   Does any learned relationship clash with your intuition?
-   Can summarize individual and interactive importance of the predictors
-   Can provide global summary of overall relationships vs local explanations of individual predictions
-   Will provide more details on Day 3

::: notes
-   Tools for explainable AI, or xAI available in R and python
:::

## 8. Example: Model explainer

```{r}
xgb_explainer <- explain_tidymodels(
  final_model_fit,
  data = dat_train,
  # DALEX required an integer for factors:
  y = as.integer(dat_train$event) - 1,
  verbose = TRUE
)
```

::: notes
-   Create a model explainer that will run the
:::

## 8. Example: PDP

-   Partial dependence profiles show relationship between predicted probabilities and the values of individual predictors.

::: notes
:::

------------------------------------------------------------------------

PDP grouped by treatment arm

::: panel-tabset
### Plots 1 - 2

```{r}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Age"
#|   - "Karnofsky"

# https://ema.drwhy.ai/partialDependenceProfiles.html
plot_pdp <- function(dat, explainer, input_vars, group_var) {
  set.seed(86329)
  
  pdp_list <- lapply(input_vars, function(vname) {
    var_splits <- list(unique(dat[[vname]]))
    names(var_splits) <- vname
    
    model_profile(
      xgb_explainer,
      variables = vname,
      N = NULL,
      groups = group_var,
      variable_splits = var_splits
    )
  })
  
  lapply(pdp_list, plot)
}


plot_pdp(dat = dat_train, explainer = xgb_explainer, 
         input_vars='age', group_var='treatment_arm')[[1]]

plot_pdp(dat = dat_train, explainer=xgb_explainer, 
         input_vars='karnof', group_var='treatment_arm')[[1]]
```

### Plots 3 - 4

```{r}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Days on prior ART"
#|   - "Baseline CD8"

plot_pdp(dat = dat_train, explainer=xgb_explainer, 
         input_vars='days_prior_art', group_var='treatment_arm')[[1]]

plot_pdp(dat = dat_train, explainer=xgb_explainer, 
         input_vars='baseline_cd8', group_var='treatment_arm')[[1]]
```
:::

::: notes
:::

# 9. Deploy the model

## 9. Deploy the model

-   Model deployment: integrate a machine learning model and integrate it into an existing production environment where it can take in an input and return an output.

-   Make the predictions from a trained ML model available to others, whether that be users, management, or other systems.

-   Model deployment will be discussed in the hands-on session on Day 3.

::: footer
Source: <https://towardsdatascience.com/what-does-it-mean-to-deploy-a-machine-learning-model-dddb983ac416>
:::

# 10. Create a ML workflow

## 10. Create a ML workflow

-   A machine learning workflow is a fully-scripted set of tasks that complete all steps in a machine learning pipeline

-   Tasks might include

    -   data ingestion

    -   data pre-processing

    -   parameter tuning over multiple algorithms

    -   selecting the optimal model

    -   create visual summaries

    -   generate report

    -   etc.

------------------------------------------------------------------------

### Targets

-   Can leverage the [`targets`](https://cran.r-project.org/web/packages/targets/index.html "https://cran.r-project.org/web/packages/targets/index.html") framework:

    -   Analyzes the dependency relationships among the tasks of a workflow

    -   Skips steps that are already up to date

    -   Runs the necessary computation with optional parallel workers

    -   Abstracts files as R objects

    -   Provides tangible evidence that the results match the underlying code and data.

-   For details, visit the link to the `targets` manual provided in the resources section

# Closing remarks

## Closing remarks

-   In this introductory lecture, we discussed the 10 best practices of machine learning.

-   We will continue this discussion on Day 3 (October 12, 2022) for the hands-on workshop. I hope you will join us!

    -   Details about the workshop will be emailed to you.

-   I hope you feel empowered to learn more about these apply these approaches in your work at Lilly.

# Resources

## General resources

-   2022 ML Workshop

    -   [GitHub](https://github.com/sydeaka/machine-learning-workshop-2022)
    -   ACTG 175 study used in demo
        -   Data source/detail: https://www.rdocumentation.org/packages/speff2trial/versions/1.0.4/topics/ACTG175
        -   Journal article: https://www.nejm.org/doi/10.1056/NEJM199610103351501

-   Machine learning algorithms

-   TidyModels workshop: [Slides](https://workshops.tidymodels.org/); [GitHub](https://github.com/tidymodels/workshops/tree/main/classwork); [Book](https://www.tmwr.org/)

-   [Feature Engineering book](http://www.feat.engineering/)

-   Model deployment

    -   [Bike prediction demo](https://solutions.rstudio.com/example/bike_predict/)

    -   [LEGO set demo](https://juliasilge.com/blog/lego-sets/)
