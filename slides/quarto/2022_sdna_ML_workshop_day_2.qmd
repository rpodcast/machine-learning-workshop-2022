---
title: "Machine Learning: Principles & Best Practices"
author: "Sydeaka Watson"
format: 
  revealjs:
    code-link: true
    code-line-numbers: true
    code-tools: true
    code-copy: true
    code-fold: true
    code-summary: "Show the code"
    smaller: false
    incremental: false
    scrollable: true
    theme: black
    footer: "2022 SDnA Machine Learning Workshop"
    slide-number: true
    chalkboard: true
    multiplex: true
    fig-height: 8
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 10 ML best practices

::: columns
::: {.column width="50%"}
1.  Define modeling objectives

2.  Exploratory data analysis

3.  Feature engineering

4.  Variable selection

5.  Split data into training / testing sets
:::

::: {.column width="50%"}
6.  Train / validate the model

7.  Create visual summaries

8.  Explain the model predictions

9.  Deploy the model

10. Create a ML workflow
:::
:::

# 1. Define modeling objectives

## 1. Define modeling objectives

-   What do we want the model to do?

    -   Classification? Regression? Clustering?

-   What is the outcome / target variable?

-   Identify the relevant data sources

-   Select appropriate ML algorithm(s) that align with the modeling objective

## 1. Example: Clinical Trial ACTG 175

-   ACTG 175 was a randomized, double-blind, placebo-controlled trial that included HIV-1--infected subjects with CD4 cell counts 200-500 per cubic mm

-   Treatments

    -   Monotherapy: (1) zidovudine alone or (2) didanosine alone
    -   Combination therapy: (3) zidovudine + didanosine or (4) zidovudine + zalcitabine

-   Primary study end point: (1) \>50% decline in CD4 cell count, (2) progression to AIDS, or (3) death.

## 1. Example: Modeling Plan

-   Objective: Given baseline characteristics, predict whether patient will experience the primary study endpoint within the observation period

-   Outcome: Binary indicator of whether patient had \>50% CD4+ increase, progressed to AIDS, or death

-   Data source: Clinical data -- baseline predictors, treatment assignment, study outcomes

-   Candidate algorithms

    -   Supervised learning / binary classification: Random forest, XGBoost, & Logistic regression

# 2. Exploratory data analysis

## 2. Exploratory data analysis

-   Data quality

-   Missing data summary / investigation

-   Determine if we need to exclude any data points or variables

-   Determine if we need to transform any variables

-   Examine relationships between candidate predictors and outcomes

## 2. Example: Environment setup

::: panel-tabset
### Load Packages

```{r load_pkg}
#| echo: fenced
#| code-fold: false

# Load the packages
suppressPackageStartupMessages(library(speff2trial))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(testthat))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(yardstick))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(themis))
suppressPackageStartupMessages(library(DALEXtra))
```

### Load Helper Fns

```{r load_helpers}
#| echo: fenced
#| code-fold: false

# Helper function: Pretty data table
# Table fmt ref: https://stackoverflow.com/questions/44368572/dt-showing-more-rows-in-dt
pretty_dt <- function(tbl, num_digits = 3, num_rows=20) {
  dt <- tbl %>% 
    head(num_rows) %>%
    datatable(
      fillContainer = FALSE, 
      options = list(pageLength = 5, 
                     autoWidth = TRUE,
                     dom = 'tip', # table only
                     scrollX = TRUE,
                     scrollY = TRUE,
                     initComplete = JS(
                            "function(settings, json) {",
                            "$(this.api().table().header()).css({'color': 'white'});",
                            "}")
        )
      )
  
  # Round float columns (numeric but not integer)
  cols_numeric <- colnames(tbl)[sapply(tbl, is.numeric)]
  cols_integer <- colnames(tbl)[sapply(tbl, is.integer)]
  cols_float <- cols_numeric[!(cols_numeric %in% cols_integer)]
  if (length(cols_float) > 0) {
    dt <- dt %>% formatRound(cols_float, num_digits)
  }
  
  return(dt)
}
```

### Load Data

```{r load_data}
#| echo: fenced

# Load the data
data(ACTG175)
dat_raw <- ACTG175
dat_raw %>% summary
```
:::

## 2. Example: Data transformations

```{r data_transformations}
dat_clean <- dat_raw %>%
  mutate(treatment_arm = case_when(
    arms == 0 ~ 'zidovudine',
    arms == 1 ~ 'zidovudine and didanosine',
    arms == 2 ~ 'zidovudine and zalcitabine',
    arms == 3 ~ 'didanosine',
    TRUE ~ NA_character_
  )) %>%
  mutate(
    hemo = case_when(
      hemo == 0 ~ 'no',
      hemo == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    lgbtq = case_when(
      homo == 0 ~ 'no',
      homo == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    hist_intra_drug_use = case_when(
      drugs == 0 ~ 'no',
      drugs == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_nz_art = case_when(
      oprior == 0 ~ 'no',
      oprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_z_30days = case_when(
      z30 == 0 ~ 'no',
      z30 == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_z = case_when(
      zprior == 0 ~ 'no',
      zprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    days_prior_art = preanti,
    race = case_when(
      race == 0 ~ 'white',
      race == 1 ~ 'non-white',
      TRUE ~ NA_character_
    ),
    gender = case_when(
      gender == 0 ~ 'female',
      gender == 1 ~ 'male',
      TRUE ~ NA_character_
    ),
    prior_art = case_when(
      str2 == 0 ~ 'naive',
      str2 == 1 ~ 'experienced',
      TRUE ~ NA_character_
    ),
    strat_hist_art = case_when(
      strat == 1 ~ 'antiretroviral naive',
      strat == 2 ~ '> 1 but <= 52 weeks of prior antiretroviral therapy',
      strat == 3 ~ '> 52 weeks',
      TRUE ~ NA_character_
    ),
    symptom = case_when(
      symptom == 0 ~ 'asymptomatic',
      symptom == 1 ~ 'symptomatic',
      TRUE ~ NA_character_
    ),
    prior_z = case_when(
      zprior == 0 ~ 'no',
      zprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    prior_z = case_when(
      zprior == 0 ~ 'no',
      zprior == 1 ~ 'yes',
      TRUE ~ NA_character_
    ),
    zidovudine_indicator = treat,
    surv_days = days,
    surv_event = cens,
    event = ifelse(surv_event == 0, 'no', 'yes') %>% factor,
    baseline_cd4 = cd40,
    baseline_cd8 = cd80
  ) %>%
  select(
    # identifier
    pidnum, 
    # treatment assignment 
   zidovudine_indicator, treatment_arm, offtrt, 
    # baseline predictors
    age, wtkg, hemo, lgbtq, hist_intra_drug_use, karnof, prior_nz_art, prior_z_30days, prior_z, days_prior_art, race, gender, prior_art, strat_hist_art, symptom, baseline_cd4, baseline_cd8, 
   # survival outcome
   event, surv_event, surv_days,
   # other outcomes
   cd420, cd496, r, cd820
    ) %>%
  mutate_if(is.character, factor) 

dat_clean %>% summary
```

## 2. Example: Data preview

```{r data_preview}
dat_clean %>% pretty_dt
```

## 2. Example: Data visualizations

::: panel-tabset
### Plots 1 - 2

```{r data-data_viz_p12, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Event distribution is skewed"
#|   - "Baseline CD4 levels extend beyond the 200-500 range"

# Specify variable types
var_outcomes <- c('event', 'surv_event', 'surv_days', 'cd420', 'cd496', 'r', 'cd820')
excluded_cols <- c('pidnum', 'zidovudine_indicator', 'offtrt')
classes <- sapply(dat_clean, class)
vars_factor <- classes[classes == 'factor'] %>% 
  names %>% 
  .[!(. %in% c('progression', 'event'))]
vars_cont <- classes[(classes  %in% c('integer', 'numeric'))] %>% 
  names %>% 
  .[!(. %in% c(var_outcomes, excluded_cols))]


# Bar plot of event counts
dat_clean %>%
  count(event) %>%
  ggplot(aes(x = event, y = n)) + 
  geom_bar(stat = 'identity') + 
  ylab('') + ggtitle('Patient counts by event status')


# Boxplot of baseline CD4 levels across treatment arms
dat_clean %>%
  ggplot(aes(x = treatment_arm, y = baseline_cd4)) + 
  geom_boxplot() + 
  coord_flip() +
  xlab('') + ylab('') + ggtitle('Baseline CD4')
```

### Plots 3 - 4

```{r data_viz_p34, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "All patients have had prior zidovudine therapy "
#|   - "Few patients have had prior non-zidovudine therapy "

# Bar plot of prior_z counts
dat_clean %>%
  count(prior_z) %>%
  ggplot(aes(x = prior_z, y = n)) + 
  geom_bar(stat = 'identity') +
  xlab('') + ylab('') + ggtitle('Patient counts by prior zidovudine history')


# Bar plot of prior_nz_art counts
dat_clean %>%
  count(prior_nz_art) %>%
  ggplot(aes(x = prior_nz_art, y = n)) + 
  geom_bar(stat = 'identity') +
  xlab('') + ylab('') + ggtitle('Patient counts by prior non-zidovudine history')
```
:::

# 3. Feature engineering

## 3. Feature engineering

-   Create new features that extract additional insight from existing features

-   For example, we could split timestamp "2022-09-23 11:37:48 CDT" into:

    ::: columns
    ::: {.column width="50%"}
    -   year = 2022

    -   quarter = 3

    -   month = 9

    -   day_of_month = 23

    -   day_of_week = 'Friday'
    :::

    ::: {.column width="50%"}
    -   hour_24 = 11

    -   minutes = 37

    -   seconds = 48

    -   time_zone = 'CDT'
    :::
    :::

## 3. Example: Individual drugs

```{r feat_eng}
dat_clean <- dat_clean %>%
  mutate(
    ind_didanosine = ifelse(grepl(pattern='didanosine', x=treatment_arm), 1L, 0L),
    ind_zidovudine = ifelse(grepl(pattern='zidovudine', x=treatment_arm), 1L, 0L),
    ind_zalcitabine = ifelse(grepl(pattern='zalcitabine', x=treatment_arm), 1L, 0L)
  )

dat_clean %>%
  distinct(treatment_arm, ind_didanosine, ind_zidovudine, ind_zalcitabine) %>%
  pretty_dt
```

# 4. Variable selection

## 4. Variable selection

-   Which variables do we want to include in the analysis?

-   Which should be excluded?

    -   Identifier fields, redundant variables, zero-variance variables, etc.

    -   Keep all variables and let the algorithm figure it out? Or feed it specific variables?

-   Variable reduction - principal components

-   Could also choose n top variables from preliminary model fit

## 4. Example: Keeping these variables

-   Treatment arm assignments (factor + binary indicators)

-   Baseline characteristics

    -   Age, weight, race, gender, sexual orientation, etc.

    -   Treatment history (including stratification variable)

    -   CD4 and CD8 counts

-   Symptomatic vs asymptomatic?

-   Karnofsky score

## 4. Example: Excluding these variables

-   [**pidnum**]{.underline}: patient identifier shouldn't be used as a predictor

-   [**offtrt**]{.underline}: we wouldn't know this at baseline (whether the patient went off treatment)

-   [**prior_z**]{.underline}: all patients have prior zidovudine treatment (i.e., all patients have the same value for this variable), so there's no value add

-   [**prior_nz_art**]{.underline}: Very little variability in this variable (2092 no vs 47 yes)

-   [**CD4 and CD8 measurements at later time points**]{.underline}: we wouldn't know these at baseline

## 4. Example: Analysis subset

```{r var_types}
var_predictors <- c(
    # treatment assignment 
    'treatment_arm', 
    'ind_didanosine', 'ind_zidovudine', 'ind_zalcitabine',
    # baseline predictors
    'age', 'wtkg', 'hemo', 'lgbtq', 'hist_intra_drug_use', 'karnof', 'prior_z_30days', 
    'days_prior_art', 'race', 'gender', 'prior_art', 'strat_hist_art', 'symptom', 'baseline_cd4', 'baseline_cd8')

var_outcomes <- c('event', 'surv_event', 'surv_days', 'cd420', 'cd496', 'r', 'cd820')
vars_all <- colnames(dat_clean)
vars_to_keep <- c(var_predictors, var_outcomes)
vars_excluded <- vars_all[!(vars_all %in% vars_to_keep)]

dat_analysis <- dat_clean %>% select_at(vars_to_keep)

dat_analysis %>% summary
```

# 5. Split data into training / testing

## 5. Split data into training / testing

-   Fit the model on the training set(s)

-   Get intermediate assessment of model performance on validation set(s)

-   Use k-fold cross-validation to train/validate on multiple data splits

-   Get final assessment of model performance on testing set

## 5. Example: Train /validation/test split

### Initial data split

85 % training / validation + 15% test

```{r dat_split}
# Set the seed so the result is reproducible
set.seed(123)
# 85% training / 15% testing
dat_split <- initial_split(dat_analysis, prop = 0.85, strata = strat_hist_art)
dat_split
```

```{r dat_split_dims}
dat_train_and_val <- training(dat_split)
dat_test <- testing(dat_split)

df_dimensions <- list(
  full_dataset = dat_analysis %>% dim,
  train_val_dataset = dat_train_and_val %>% dim,
  test_dataset = dat_test %>% dim
)

df_dim_message <- sapply(names(df_dimensions), function(df_name) {
  glue::glue("`{df_name}` contains {df_dimensions[[df_name]][1]} rows and {df_dimensions[[df_name]][2]} columns.")
}) %>%
  paste(., collapse='\n')

cat(df_dim_message)
```

------------------------------------------------------------------------

### Correct class imbalance issue

-   There are 3x as many 'no' events as there are 'yes' events.

-   This can negatively impact model performance.

```{r}
# Show imbalanced distribution of event outcome
dat_train_and_val %>% count(event)
```

------------------------------------------------------------------------

### Correct class imbalance issue

-   We can use the `step_upsample` function to over-sample the minority class (i.e., the 'yes' class).

-   This creates replicate rows of the 'yes' records that can artificially make the proportions more balanced

-   We would only do this for the training data

```{r}
# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Recipe for upsampling the minority class
upsample_rec <- 
  recipe(model_formula, data = dat_train_and_val) %>%
  step_upsample(event, over_ratio = 0.7) %>%
  prep()

# Apply the upsampling
dat_train_and_val <- upsample_rec %>%
  bake(new_data = NULL) 

# Show new distrubution of event outcome
dat_train_and_val %>%  count(event)
```

------------------------------------------------------------------------

### Validation split

80% training + 20% validation

```{r dat_val_split}
set.seed(321)
dat_val_split <- validation_split(dat_train_and_val, prop = 0.80, strata = strat_hist_art)

dat_train <- analysis(dat_val_split$splits[[1]])
dat_valid <- assessment(dat_val_split$splits[[1]])

df_dimensions <- list(
  dat_train_and_val_dataset = dat_train_and_val %>% dim,
  train_dataset = dat_train %>% dim,
  valid_dataset = dat_valid %>% dim
)


df_dim_message <- sapply(names(df_dimensions), function(df_name) {
  glue::glue("`{df_name}` contains {df_dimensions[[df_name]][1]} rows and {df_dimensions[[df_name]][2]} columns.")
}) %>%
  paste(., collapse='\n')

cat(df_dim_message)
```

# 6. Train the model(s)

## 6. Train the model(s)

-   Model = modeling algorithm + model formula + algorithm hyperparameter values

    -   Try different modeling algorithms

    -   Try different hyperparameter values

    -   May need to use parallel processing over multiple cores for faster training

-   Model validation - evaluate each model on holdout dataset(s)

    -   Good: Train on training set / validate on validation set

    -   Best: Cross-validation on training/validation set

## 6a. Example: Benchmark model

We will compare ML model performance to this basic logistic regression fit.

```{r}
# Model specification
model_spec_logreg <- logistic_reg(mode = "classification", engine = "glm")

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflow
logreg_model_wflow <- workflow(model_formula, model_spec_logreg)

# Train the model
model_fit_logreg <- fit(logreg_model_wflow, dat_train)

# Append the predictions to the testing dataframe
dat_valid_w_preds_logreg <- augment(model_fit_logreg, new_data = dat_valid) %>%
  mutate(.pred_yes_int = ifelse(.pred_yes > .5, 1, 0))

# Show the augmented validation dataframe (original data + predictions)
dat_valid_w_preds_logreg %>% pretty_dt
```

## 6a. Example: Benchmark model

Logistic regression model accuracy

```{r}
# Get model performance: Accuracy
dat_valid_w_preds_logreg %>%
  yardstick::accuracy(truth=event, estimate=.pred_class) %>%
  pretty_dt
```

## 6a. Example: Benchmark model

Area under ROC Curve for logistic regression model fit

```{r}
# Get model performance: Area under the ROC curve
dat_valid_w_preds_logreg %>%
  yardstick::roc_auc(truth=event, estimate=.pred_yes, event_level='second') %>%
  pretty_dt
```

## 6a. Example: Benchmark model

Confusion matrix: Logistic regression model struggles to correctly predict events.

```{r}
dat_valid_w_preds_logreg %>%
  conf_mat(truth=event, estimate=.pred_class) %>%
  autoplot(type='heatmap')
```

## 6b. Example: Set up parallel backend

-   Machine learning algorithms can be computationally expensive

-   Parallelizing these computations over multiple cores can significantly reduce training time

-   Code below shows an example parallel backend setup

```{r parallel_setup}
#| echo: fenced
#| code-fold: false

cores <- parallelly::availableCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)
```

## 6c. Example: RF k-fold CV

Train a Random Forest model using k-fold cross-validation

-   Same model formula used in the logistic regression (benchmark model)

-   Using the Random Forest implementation provided in the `ranger` package

-   Setting specific values for selected hyperparameters (`trees`, `mtry`, and `min_n`)

------------------------------------------------------------------------

### Example: RF k-fold CV

-   In each step, fit models on (k-1) folds combined and evaluate performance on remaining fold

-   Fold splits are stratified by patient's history of anti-retroviral therapy

-   Saving predictions on holdout folds for later use

-   Default performance metrics for binary classification: accuracy + AUC

------------------------------------------------------------------------

### Code

```{r rf_train_w_cv}
#| echo: fenced
#| code-fold: false

# Generate k folds for CV
num_folds <- 5
dat_folds <- vfold_cv(dat_train_and_val, v = num_folds, strata = strat_hist_art)

# Save the assessment set results
ctrl <- control_resamples(save_pred = TRUE)

# Model specification
model_spec_rf <- rand_forest(mode = 'classification', engine = 'ranger', 
                             trees = 300, mtry = 10, min_n = 100)

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflow
rf_model_wflow <- workflow(model_formula, model_spec_rf)

# k-fold cross validation of the random forest model
fit_cv_res <- fit_resamples(rf_model_wflow, dat_folds, control = ctrl)
```

## 6c. RF k-fold CV

-   Evaluate model performance

    -   Aggregate performance metrics across folds

        -   Shows mean, standard error, and n = \# folds

```{r}
fit_cv_res %>% 
  collect_metrics() %>%
  pretty_dt()
```

## 6c. RF k-fold CV

-   Get predictions on the held-out folds

    -   Predicted probability of yes or no

    -   Predicted class = whichever has highest predicted probability

    -   Also includes the true classification column `event` for comparison purposes

------------------------------------------------------------------------

```{r}
fit_cv_res %>% 
  collect_predictions() %>%
  pretty_dt()
```

## 6d. Example: Multiple models

-   Fit Random forest, XGBoost, and Logistic regression models in a single workflow

-   Can test various model formula and algorithm combinations

```{r multi_model_train_w_cv}
#| echo: fenced
#| code-fold: false

# Generate k folds for CV
num_folds <- 3
dat_folds <- vfold_cv(dat_train_and_val, v = num_folds, strata = strat_hist_art)

# Save the assessment set results
ctrl <- control_resamples(save_pred = TRUE)

# Model specifications
model_spec_rf <- rand_forest(mode = 'classification', engine = 'ranger', trees = 300, 
                             mtry = 10, min_n = 25)
#model_spec_dt <- decision_tree(mode = 'classification', engine = 'rpart', tree_depth = 5, 
#                               min_n = 20)
model_spec_xgb <- boost_tree(mode = 'classification', engine = 'xgboost', trees = 250,
                             tree_depth = 5, min_n = 30, learn_rate = 0.5)
model_spec_logreg <- logistic_reg(mode = "classification", engine = "glm")

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflows
rf_model_wflow <- workflow(model_formula, model_spec_rf)
#dt_model_wflow <- workflow(model_formula, model_spec_dt)
xgb_model_wflow <- workflow(model_formula, model_spec_xgb)
logreg_model_wflow <- workflow(model_formula, model_spec_logreg)

wf_set <- workflow_set(list(model_formula), 
                       list(model_spec_logreg, model_spec_rf, model_spec_xgb))

wf_set_fit <- wf_set %>%
  workflow_map("fit_resamples", resamples = dat_folds)
```

## 6d. Example: Multiple models

Leaderboard shows model performance rankings across modeling scenarios

```{r}
# Rank the sets of models by their aggregate metric performance
leaderboard <- wf_set_fit %>% rank_results()
leaderboard %>% pretty_dt()
```

## 6e. Example --- Param tuning w/ CV

Tune hyperparameters

-   Fit the same algorithm multiple times using different parameter values

-   Choose the parameter combination that optimizes model performance

-   Can choose grid search approach - Latin Hypercube, Max entropy, Regular, etc

-   Can choose parameter value ranges or have them automatically selected

------------------------------------------------------------------------

### Code

```{r xgb_param_tune_w_cv}
#| echo: fenced
#| code-fold: false

set.seed(9265)

# Generate k folds for CV
num_folds <- 5
dat_folds <- vfold_cv(dat_train_and_val, v = num_folds, strata = strat_hist_art)

# Model specifications
# Some parameters are specified, while others are marked for tuning 
model_spec_xgb_tune <- boost_tree(mode = 'classification', engine = 'xgboost', trees = tune(),
                             tree_depth = tune(), min_n = 54, learn_rate = tune(), 
                             mtry = tune())

# Save the assessment set results
ctrl <- control_grid(save_pred = TRUE, allow_par=TRUE, parallel_over = "everything", 
                     verbose=TRUE, event_level='second')

# Model formula
model_formula <- as.formula(paste('event ~ ', paste(var_predictors, collapse = ' + ')))

# Model workflow
xgb_model_wflow_tune <- workflow(model_formula, model_spec_xgb_tune)

xgb_param <- 
  xgb_model_wflow_tune %>% 
  extract_parameter_set_dials() %>% 
  update(
    trees = trees(c(100, 1500)),
    learn_rate = learn_rate(c(.00005, .1), trans= NULL),
    tree_depth = tree_depth(c(4, 20)),
    mtry = mtry(c(3, 20))
  )

# Tune the hyperparameters using k-fold cross validation
set.seed(9)
tm <- system.time({
  xgb_res <-
    xgb_model_wflow_tune %>%
    tune_grid(resamples = dat_folds, 
              grid = xgb_param %>% grid_max_entropy(size = 100),
              control = ctrl)
})

# How long did it take to run?
print(tm)
```

## 6e. Example --- Param tuning w/ CV

Summarize model performance metrics across folds

```{r xgb_metrics}
xgb_res %>% 
  collect_metrics() %>%
  pretty_dt
```

## 6e. Example --- Param tuning w/ CV

Show the top XGBoost model

```{r xgb_top_model}
# top model
xgb_res %>% 
  show_best(metric = 'roc_auc', n=1) %>% 
  pretty_dt
```

## 6e. Example --- Stop the cluster

Shut down the parallel backend

```{r parallel_close}
#| echo: fenced
#| code-fold: false

foreach::registerDoSEQ()
parallel::stopCluster(cl)
```

# 7. Create visual summaries

## 7. Create visual summaries

Visual model summaries can often provide additional insight

-   Compare model performance across modeling scenarios

-   Compare predicted values to observed

    -   Regression: scatterplot

    -   Classification: confusion matrix

## 7. Example: Model performance across tuning scenarios

```{r xgb_tune_metrics, fig.height=6}
xgb_tune_metrics <- xgb_res %>% 
  collect_metrics() %>%
  mutate(scenario = glue::glue("learn_rate = {signif(learn_rate, 3)}; trees = {trees}; tree_depth = {tree_depth}"))

plot_tune_metrics_xgb <- xgb_tune_metrics %>%
  #filter(.metric == 'roc_auc') %>% 
  ggplot(aes(x=scenario)) + 
  geom_point(aes(x=scenario, y=mean)) +
  geom_errorbar(aes(ymin=mean-std_err, ymax=mean+std_err), width=.2,
                 position=position_dodge(0.05)) +
  facet_wrap(~.metric) +
  coord_flip()

plot_tune_metrics_xgb
```

## 7. Example: ROC vs param value

-   Plotting the model performance against the parameter values in the grid can often reveal trends that can help refine the grid for better performance.

-   For example... \<insight\>

------------------------------------------------------------------------

### Plots: Learning rate and num. trees

```{r, fig.width=10, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Learning rate"
#|   - "Number of trees"

plot_param <- function(param) {
  xgb_tune_metrics %>%
    filter(.metric == 'roc_auc') %>%
    arrange_at('mean') %>%
    ggplot(aes_string(x=param, y='mean')) +
    geom_point() + 
    xlab(param) + 
    ylab('') + 
    ggtitle(glue::glue("ROC vs {param}"))
}

plot_param('learn_rate')
plot_param('trees')
```

------------------------------------------------------------------------

### Plots: Tree depth and mtry

```{r, fig.width=10, fig.height=6}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Tree depth"
#|   - "Mtry"

plot_param('tree_depth')
plot_param('mtry')
```

## 7. Example: Confusion matrix

Compare predicted vs observed outcomes in a single CV fold

```{r}
cm <- xgb_res%>%
  collect_predictions() %>%
  filter(id == "Fold1") %>%
  conf_mat(event, .pred_class)

#autoplot(cm, type = "heatmap")
cm
```

## 7. Example: Confusion matrix

Summarize the predicted vs observed outcomes across k folds

```{r}
library(tidyr)

tune_preds <- xgb_res %>%
  collect_predictions()

cells_per_resample <- tune_preds %>%
  group_by(id) %>%
  conf_mat(event, .pred_class) %>%
  mutate(tidied = lapply(conf_mat, tidy)) %>%
  unnest(tidied)

# Get the totals per resample
counts_per_resample <- tune_preds %>%
  group_by(id) %>%
  summarize(total = n()) %>%
  left_join(cells_per_resample, by = "id") %>%
  # Compute the proportions
  mutate(prop = value/total) %>%
  group_by(name) %>%
  # Average
  summarize(prop = mean(prop))

# Now reshape these into a matrix
mean_cmat <- matrix(counts_per_resample$prop, byrow = TRUE, ncol = 2)
rownames(mean_cmat) <- levels(tune_preds$event)
colnames(mean_cmat) <- levels(tune_preds$event)

round(mean_cmat, 3)
```

## 7. Example: Select the best model

-   Best model is the one that optimizes the performance metric

-   For example, we could choose the model with highest AUC

------------------------------------------------------------------------

### Code

```{r}
## XGBoost model that has the best AUC
best_auc <- select_best(xgb_res, metric = "roc_auc")

## Workflow associated with the best AUC
best_xgb_wflow <- xgb_model_wflow_tune %>% 
  finalize_workflow(best_auc)

## The final model fit
final_model_fit <- 
  best_xgb_wflow %>% 
  fit(data = dat_train)

print(final_model_fit)
```

# 8. Explain model predictions

## 8. Explain model predictions

-   Understand the patterns that the model has learned
-   What does it understand about the relationship between the outcome and predictors?
    -   Any new insights?
    -   Does any learned relationship clash with your intuition?
-   Can summarize individual and interactive importance of the predictors
-   Can provide global summary of overall relationships vs local explanations of individual predictions
-   Will provide more details on Day 3

## 8. Example: Model explainer

```{r}
xgb_explainer <- explain_tidymodels(
  final_model_fit,
  data = dat_train,
  # DALEX required an integer for factors:
  y = as.integer(dat_train$event) - 1,
  verbose = TRUE
)
```

## 8. Example: PDP

-   Partial dependence profiles show relationship between predicted probabilities and the values of individual predictors.

------------------------------------------------------------------------

PDP grouped by treatment arm

::: panel-tabset
### Plots 1 - 2

```{r}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Age"
#|   - "Karnofsky"

# https://ema.drwhy.ai/partialDependenceProfiles.html
plot_pdp <- function(dat, explainer, input_vars, group_var) {
  set.seed(86329)
  
  pdp_list <- lapply(input_vars, function(vname) {
    var_splits <- list(unique(dat[[vname]]))
    names(var_splits) <- vname
    
    model_profile(
      xgb_explainer,
      variables = vname,
      N = NULL,
      groups = group_var,
      variable_splits = var_splits
    )
  })
  
  lapply(pdp_list, plot)
}


plot_pdp(dat = dat_train, explainer = xgb_explainer, 
         input_vars='age', group_var='treatment_arm')[[1]]

plot_pdp(dat = dat_train, explainer=xgb_explainer, 
         input_vars='karnof', group_var='treatment_arm')[[1]]
```

### Plots 3 - 4

```{r}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Days on prior ART"
#|   - "Baseline CD8"

plot_pdp(dat = dat_train, explainer=xgb_explainer, 
         input_vars='days_prior_art', group_var='treatment_arm')[[1]]

plot_pdp(dat = dat_train, explainer=xgb_explainer, 
         input_vars='baseline_cd8', group_var='treatment_arm')[[1]]
```
:::

# 9. Deploy the model

## 9. Deploy the model

Meodel deployment will be discussed in the hands-on session on Day 3.

# 10. Create a ML workflow

## 10. Create a ML workflow

-   A machine learning workflow is a fully-scripted set of tasks that complete all steps in a machine learning pipeline

-   Tasks might include

    -   data ingestion

    -   data pre-processing

    -   parameter tuning over multiple algorithms

    -   selecting the optimal model

    -   create visual summaries

    -   generate report

    -   etc.

------------------------------------------------------------------------

### Targets

-   Can leverage the [`targets`](https://cran.r-project.org/web/packages/targets/index.html "https://cran.r-project.org/web/packages/targets/index.html") framework:

    -   Analyzes the dependency relationships among the tasks of a workflow

    -   Skips steps that are already up to date

    -   Runs the necessary computation with optional parallel workers

    -   Abstracts files as R objects

    -   Provides tangible evidence that the results match the underlying code and data.

-   For details, visit the link to the `targets` manual provided in the resources section

# Closing remarks

## Closing remarks

-   In this introductory lecture, we discussed the 10 best practices of machine learning.

-   We will continue this discussion on Day 3 (October 12, 2022) for the hands-on workshop. I hope you will join us!

    -   Details about the workshop will be emailed to you.

-   I hope you feel empowered to learn more about these apply these approaches in your work at Lilly.

# Resources

## General resources

-   2022 ML Workshop

    -   [GitHub](https://github.com/sydeaka/machine-learning-workshop-2022)

    -   [Video recording of Day 1](https://1759891.mediaspace.kaltura.com/media/2022+GSS+++SDnA+ML+Workshop+Day+1A+Machine+Learning+at+Lilly/1_2t52m5zn)

-   Machine learning algorithms

-   TidyModels workshop: [Slides](https://workshops.tidymodels.org/); [GitHub](https://github.com/tidymodels/workshops/tree/main/classwork); [Book](https://www.tmwr.org/)

-   [Feature Engineering book](http://www.feat.engineering/)

-   Model deployment

    -   [Bike prediction demo](https://solutions.rstudio.com/example/bike_predict/)

    -   [LEGO set demo](https://juliasilge.com/blog/lego-sets/)

## Lilly resources

-   HPC documentation

    -   [RIDS HPC](https://hpc.am.lilly.com/index.php/Main_Page)

    -   [HPC Analytics Documentation Portal](http://hpc-analytics.am.lilly.com)

-   JupyterHub: [Link](https://jupyterhub.am.lilly.com/); [Documentation](https://hpc.am.lilly.com/index.php/JupyterHub)

-   RStudio Server: [Link](https://rstudio.am.lilly.com/); [Documentation](http://hpc-analytics.am.lilly.com/r_info/rstudio_workbench/)

-   RStudio Connect: [Link](https://rstudio-connect.am.lilly.com/); [Documentation](http://hpc-analytics.am.lilly.com/r_info/rsconnect/)

-   Analytics WorkBench: Link; Documentation

-   Targets workflow: [CRAN](https://cran.r-project.org/web/packages/targets/index.html), [Manual](https://books.ropensci.org/targets/), [Vignette](https://cran.r-project.org/web/packages/targets/vignettes/overview.html)
